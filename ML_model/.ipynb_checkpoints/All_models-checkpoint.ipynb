{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715177ed",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f685c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neptune as neptune\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69616bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pandas dataframe with features for training model\n",
    "df = pd.read_pickle('dataforml.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.last_phdos_peak.hist(bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude extreme outliers from dataset using phonon freqeuncy as criteria\n",
    "df=df[df['last_phdos_peak']<1700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb78820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate targets and features\n",
    "y=df.iloc[:,-1] #targets\n",
    "X=df.iloc[:,:-2] #features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb252a3a",
   "metadata": {},
   "source": [
    "### Dimension reduction (drop redundant features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop constant features (low variance)\n",
    "features_constant = X.loc[:,X.nunique() == 1].columns.to_list()\n",
    "X.drop(columns=features_constant,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4119d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop intercorrelated features\n",
    "correlation_matrix = X.corr()\n",
    "correlated_features = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.append(colname)\n",
    "X.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d32262",
   "metadata": {},
   "source": [
    "### Start modelling section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with model names that will be trained and evaluated\n",
    "models_names =['LinearRegression', 'Ridge', 'Lasso', 'RandomForestRegressor', \n",
    "               'KernelRidge', 'GradientBoostingRegressor']\n",
    "\n",
    "# Dict with model instances with hyperparameters for gridsearchCV to optimize and validate models\n",
    "models_dicts = {LinearRegression():{'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__fit_intercept': [True, False], # LinearRegression model parameters\n",
    "    'regressor__positive':[True, False], # LinearRegression model parameters\n",
    "                      },\n",
    "    Ridge():{'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__fit_intercept': [True, False], # RidgeRegression model parameters\n",
    "    'regressor__positive':[True, False], # RidgeRegression model parameters\n",
    "    'regressor__alpha': [0.1, 0.5, 0.8, 1.0, 5, 10, 15], # RidgeRegression model parameters\n",
    "}, \n",
    "    Lasso(): {'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__fit_intercept': [True, False], # Lasso model parameters\n",
    "    'regressor__positive':[True, False], # Lasso model parameters\n",
    "    'regressor__alpha': [0.1, 0.5, 0.8, 1.0, 5, 10, 15], # Lasso model parameters\n",
    "},\n",
    "     RandomForestRegressor(n_jobs=12): {'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__n_estimators': [80,90,100,110,120,130,140,150], # RandomForestRegressor model parameters\n",
    "},\n",
    "    KernelRidge(): {'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__kernel': [\"linear\", \"rbf\", \"poly\", \"cosine\"], # KernelRidge model parameters\n",
    "    'regressor__alpha': [0.1, 0.5, 0.8, 1.0, 5, 10, 15] # KernelRidge model parameters\n",
    "},\n",
    "    GradientBoostingRegressor(): {'selector__k': [20, 40, 60, 80, 100], # Number of features to select\n",
    "    'regressor__n_estimators': [80,90,100,110,120,130,140,150], # GradientBoostingRegressor model parameters\n",
    "    'regressor__learning_rate': [0.001 ,0.1, 0.3, 0.5, 0.7, 0.9, 1] # GradientBoostingRegressor model parameters\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790570b3",
   "metadata": {},
   "source": [
    "##### `Note` : If using neptune logger to track model details , please add *project* name and *api_token* corresponding to your own account \n",
    "#### More details here :  https://docs.neptune.ai/usage/quickstart/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_models(model_names, models_dict, log_neptune=False):\n",
    "    \"\"\"\n",
    "    The function below with perform nested CV and also store the results in neptune.ai logger for each model \n",
    "    (Best model parameters, train/test rmse, r2 and validation plots).\n",
    "    \n",
    "    It will return a pands dataframe with train/test rmse, r2 and also saves interactive validation plots\n",
    "    \"\"\"\n",
    "    \n",
    "    for name, (model, param) in zip(models_names, models_dicts.items()):\n",
    "        \n",
    "        df = pd.DataFrame(index=[name])\n",
    "    \n",
    "        scorer = {'RMSE':'neg_root_mean_squared_error', 'r2': 'r2'}\n",
    "\n",
    "        cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        # enumerate splits\n",
    "        outer_test_rmse = []\n",
    "        outer_train_rmse =[]\n",
    "\n",
    "        outer_test_r2 = []\n",
    "        outer_train_r2 =[]\n",
    "        test_errors=[]\n",
    "        train_errors=[]\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            # split data\n",
    "            X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "            y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "            # configure the cross-validation procedure\n",
    "            cv_inner = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            # define the model\n",
    "            pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_regression)),\n",
    "            ('regressor', model)\n",
    "            ])\n",
    "\n",
    "            # define search space\n",
    "            hyperparameters = param\n",
    "            # define search\n",
    "            search = GridSearchCV(pipeline, param_grid=hyperparameters, scoring=scorer,\n",
    "                                                                                 cv=cv_inner, \n",
    "                                  refit='RMSE', return_train_score=True)\n",
    "            # execute search\n",
    "            result = search.fit(X_train, y_train)\n",
    "            # get the best performing model fit on the whole training set\n",
    "            best_model = result.best_estimator_\n",
    "\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            # evaluate model on the hold out dataset\n",
    "            yhat = best_model.predict(X_test)\n",
    "            # evaluate the model\n",
    "            rmse_test = mean_squared_error(y_test, yhat, squared=False)\n",
    "            rmse_train = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "\n",
    "            r2_test = r2_score(y_test, yhat)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            test_error = abs(y_test - yhat)\n",
    "            train_error = abs(y_train - y_train_pred)\n",
    "\n",
    "            # store the result\n",
    "            outer_test_rmse.append(rmse_test)\n",
    "            outer_train_rmse.append(rmse_train)\n",
    "\n",
    "            outer_test_r2.append(r2_test)\n",
    "            outer_train_r2.append(r2_train)\n",
    "\n",
    "            test_errors.append(test_error)\n",
    "            train_errors.append(train_error)\n",
    "\n",
    "        error_data_test = pd.concat(test_errors)\n",
    "        error_data_train = pd.concat(train_errors)\n",
    "\n",
    "        #Extract features selected along with scores obtained using SelectKBest(f_regression) in pipeline \n",
    "        feature_score=[]\n",
    "        feature_names=[]\n",
    "\n",
    "        for ind, (i, v) in enumerate(zip(search.best_estimator_.steps[1][1].scores_, \n",
    "                        search.best_estimator_.steps[1][1].get_support())):\n",
    "            if v == True:\n",
    "                feature_score.append(i)\n",
    "                feature_names.append(X.columns[ind].split('|')[-1])\n",
    "\n",
    "        # Create and save plot to neptune logger        \n",
    "        fig_feat = go.Figure(data=go.Bar(\n",
    "            x=feature_score,\n",
    "            y=feature_names,\n",
    "            orientation='h',\n",
    "        ))\n",
    "        fig_feat.update_layout(yaxis = dict(tickfont = dict(size=11)))\n",
    "        fig_feat.update_layout(xaxis = dict(tickfont = dict(size=11)))\n",
    "        fig_feat.update_yaxes(title_font=dict(size=22), color='black')\n",
    "        fig_feat.update_xaxes(title_font=dict(size=22), color='black')\n",
    "        fig_feat.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "        fig_feat.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "        fig_feat.update_xaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "        fig_feat.update_yaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "        fig_feat.update_layout(template='simple_white')\n",
    "        fig_feat.update_layout(width=1000, height =1000)\n",
    "        fig_feat.update_layout(title_text='Feature scores', title_x=0.5)\n",
    "        fig_feat.write_html(\"./{}_features.html\".format(name),include_mathjax = 'cdn')\n",
    "\n",
    "        # box plot for train and test errors\n",
    "\n",
    "        fig_val = go.Figure()\n",
    "\n",
    "        fig_val.add_trace(go.Violin(x0=name,\n",
    "                                y=error_data_test.values,\n",
    "                                legendgroup='Test',name='Test',\n",
    "                                side='positive', #scalegroup='Train',\n",
    "                                line_color='blue', box_visible=True)\n",
    "                     )\n",
    "        fig_val.add_trace(go.Violin(x0=name,\n",
    "                                y=error_data_train.values,\n",
    "                                legendgroup='Train',name='Train',\n",
    "                                side='negative', #scalegroup='Test',\n",
    "                                line_color='orange', box_visible=True)\n",
    "                     )\n",
    "        fig_val.update_traces(meanline_visible=True)\n",
    "        fig_val.update_layout(violingap=0, violinmode='overlay')\n",
    "        fig_val.update_traces(marker_opacity=0.75)\n",
    "        fig_val.update_layout(yaxis = dict(tickfont = dict(size=11)))\n",
    "        fig_val.update_layout(xaxis = dict(tickfont = dict(size=11)))\n",
    "        fig_val.update_yaxes(title_font=dict(size=22), color='black')\n",
    "        fig_val.update_xaxes(title_font=dict(size=22), color='black')\n",
    "        fig_val.update_layout(width=1000, height =1000)\n",
    "        fig_val.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "        fig_val.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "        fig_val.update_xaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "        fig_val.update_yaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "        fig_val.update_layout(yaxis = dict(tickfont = dict(size=18)))\n",
    "        fig_val.update_layout(yaxis_title=\"Validation Absolute error\")\n",
    "        fig_val.update_layout(yaxis = dict(tickfont = dict(size=18)))\n",
    "        fig_val.update_layout(xaxis = dict(tickfont = dict(size=18)))\n",
    "        fig_val.update_layout(template='simple_white')\n",
    "        fig_val.update_layout(yaxis_zeroline=False)\n",
    "        fig_val.write_html(\"./{}_validation.html\".format(name),include_mathjax = 'cdn')\n",
    "\n",
    "        if log_neptune:\n",
    "            #add your neptune logger project and api_token\n",
    "            run = neptune.init_run(\n",
    "                project=\"username/nameofprojet\", name = name,\n",
    "                api_token=\"xxxx\",\n",
    "            )\n",
    "\n",
    "            # log train set metric in neptune\n",
    "            run[\"model/mean_train_rmse\"] = np.mean(outer_train_rmse)\n",
    "            run[\"model/mean_train_r2\"] = np.mean(outer_train_r2)\n",
    "\n",
    "            run[\"model/std_train_rmse\"] = np.std(outer_train_rmse)\n",
    "            run[\"model/std_train_r2\"] = np.std(outer_train_r2)\n",
    "\n",
    "            #log test set metric in neptune\n",
    "            run[\"model/mean_test_rmse\"] = np.mean(outer_test_rmse)\n",
    "            run[\"model/mean_test_r2\"] = np.mean(outer_test_r2)\n",
    "\n",
    "            run[\"model/std_test_rmse\"] = np.std(outer_test_rmse)\n",
    "            run[\"model/std_test_r2\"] = np.std(outer_test_r2)\n",
    "\n",
    "            #log best parameters for model in neptune\n",
    "            run['model/parameters']= search.best_params_\n",
    "\n",
    "            run[\"preprocessing/features_score_plot\"].upload(neptune.types.File.as_html(fig_feat))\n",
    "            run['preprocessing/features_selected'] = dict(zip(feature_names, feature_score))\n",
    "\n",
    "\n",
    "            run['validation/violinplot'].upload(neptune.types.File.as_html(fig_val))\n",
    "            run.stop()\n",
    "\n",
    "\n",
    "        df.loc[name,'mean_train_rmse'] = np.mean(outer_train_rmse)\n",
    "        df.loc[name,'mean_test_rmse'] = np.mean(outer_test_rmse)\n",
    "        df.loc[name,'mean_train_r2'] = np.mean(outer_train_r2)\n",
    "        df.loc[name,'mean_test_r2'] = np.mean(outer_test_r2)\n",
    "        df.loc[name,'std_train_rmse'] = np.std(outer_train_rmse)\n",
    "        df.loc[name,'std_test_rmse'] = np.std(outer_test_rmse)\n",
    "        df.loc[name,'std_train_r2'] = np.std(outer_train_r2)\n",
    "        df.loc[name,'std_test_r2'] = np.std(outer_train_r2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4eeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_validate_models(model_names=models_names, models_dict=models_dicts,log_neptune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e65cda",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
