{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74060d39",
   "metadata": {},
   "source": [
    "## Load the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f936245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skrebate import MultiSURFstar\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matbench.data_ops import mean_absolute_percentage_error\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pandas dataframe with targets and features for training model\n",
    "df = pd.read_pickle('./dataforml_automatminer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10594e",
   "metadata": {},
   "source": [
    "## The code block below changes the inputs to model training based on user selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3da5d",
   "metadata": {},
   "source": [
    "**<font color='red'>Important note</font>:** \n",
    "The user first needs to select either one of the button below to reproduce the results of the associated model presented in paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block to select model training input data \n",
    "heading = widgets.HTML('<h3>Please select the model you want to train and evaluate before procceding</h3>')\n",
    "include_button = widgets.Button(description='Including LOBSTER features', \n",
    "                                layout=widgets.Layout(width='300px', height='50px'),\n",
    "                               style={'font_weight': 'bold','font_size': '16px', 'border_radius': '300px'})\n",
    "\n",
    "\n",
    "exclude_button = widgets.Button(description='Excluding LOBSTER features',\n",
    "                               layout=widgets.Layout(width='300px', height='50px'),\n",
    "                               style={'font_weight': 'bold', 'font_size': '16px','border_radius': '300px'})\n",
    "\n",
    "parent = os.getcwd() # get the directory of script\n",
    "\n",
    "\n",
    "# Define the button click event handlers\n",
    "def include_button_clicked(b):\n",
    "    '''\n",
    "    This callback function includes the ICOHP features for model training and changes the output directory\n",
    "    to store the results\n",
    "    '''\n",
    "    global y, X\n",
    "    os.chdir(parent)\n",
    "    isExist = os.path.exists('inc_icohp')\n",
    "    if not isExist:\n",
    "        os.mkdir('inc_icohp')\n",
    "        os.chdir('inc_icohp')\n",
    "    else:\n",
    "        os.chdir('inc_icohp')\n",
    "        \n",
    "    y=df.iloc[:,0] #targets\n",
    "    X=df.iloc[:,1:] #features\n",
    "    print(\"LOBSTER features will be included in the model evaluation, results will be stored in the 'inc_icohp' directory\")\n",
    "    print(\"Great! Now you have the necessary data for model training and evaluation. Run the consequent code blocks\")\n",
    "    print(\"\")\n",
    "def exclude_button_clicked(b):\n",
    "    '''\n",
    "    This callback function excludes the ICOHP features for model training and changes the output directory\n",
    "    to store the results\n",
    "    '''\n",
    "    global y, X\n",
    "    os.chdir(parent)\n",
    "    isExist = os.path.exists('exc_icohp')\n",
    "    if not isExist:\n",
    "        os.mkdir('exc_icohp')\n",
    "        os.chdir('exc_icohp')\n",
    "    else:\n",
    "        os.chdir('exc_icohp')\n",
    "    \n",
    "    y=df.iloc[:,0] #targets\n",
    "    X=df.iloc[:,1:-18] #features\n",
    "    print(\"LOBSTER features will be excluded in the model evaluation, results will be stored in the 'exc_icohp' directory\")\n",
    "    print(\"Great! Now you have the necessary data for model training and evaluation. Run the consequent code blocks\")\n",
    "    print(\"\")\n",
    "# Attach the event handlers\n",
    "include_button.on_click(include_button_clicked)\n",
    "exclude_button.on_click(exclude_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(heading)\n",
    "display(include_button)\n",
    "display(exclude_button)\n",
    "display(HTML('<div class=\"alert-warning\"><h3>Please ensure you selected either of options presented above. You will encounter error ahead as inputs necessary for model will not be instantiated if you don\\'t select either of the options</h3></div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with model name that will be trained and evaluated\n",
    "models_names =['RandomForestRegressor']\n",
    "\n",
    "# Dict with model hypterparameter\n",
    "models_dicts = {\n",
    "     RandomForestRegressor(n_jobs=24): {\n",
    "         'selector__n_features_to_select': [50, 100, 180], # Number of features to select\n",
    "        'regressor__n_estimators': [500],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_plot(gridsearchcv_obj, modelname, features, iteration):\n",
    "    '''\n",
    "    Function to save the plot of selected features by feature selection algorithm (MultiSurfstar) with its scores\n",
    "    '''\n",
    "\n",
    "    selected_feature_indices = np.argsort(-1*gridsearchcv_obj.best_estimator_.steps[0][1].feature_importances_)[:\n",
    "                                                gridsearchcv_obj.best_estimator_.steps[0][1].n_features_to_select]\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    feature_names = [features.columns[i] for i in selected_feature_indices]\n",
    "    feature_score = [gridsearchcv_obj.best_estimator_.steps[0][1].feature_importances_[i] \n",
    "              for i in selected_feature_indices]\n",
    "\n",
    "\n",
    "    # Create and save plot to neptune logger        \n",
    "    fig_feat = go.Figure(data=go.Bar(\n",
    "        x=feature_score,\n",
    "        y=feature_names,\n",
    "        orientation='h',\n",
    "    ))\n",
    "    fig_feat.update_layout(yaxis = dict(tickfont = dict(size=11)))\n",
    "    fig_feat.update_layout(xaxis = dict(tickfont = dict(size=11)))\n",
    "    fig_feat.update_yaxes(title_font=dict(size=22), color='black')\n",
    "    fig_feat.update_xaxes(title_font=dict(size=22), color='black')\n",
    "    fig_feat.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "    fig_feat.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "    fig_feat.update_xaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "    fig_feat.update_yaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "    fig_feat.update_layout(template='simple_white')\n",
    "    fig_feat.update_layout(width=1000, height =1000)\n",
    "    fig_feat.update_layout(title_text='Feature scores', title_x=0.5)\n",
    "    fig_feat.write_html(\"{}/{}_features_{}.html\".format(modelname, modelname, iteration),include_mathjax = 'cdn')\n",
    "    \n",
    "    return fig_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d287f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_plot(errors_test,errors_train,labels, modelname):\n",
    "    '''\n",
    "    Function to save the absolute error distribution violin plot\n",
    "    '''\n",
    "    \n",
    "    fig_val = go.Figure()\n",
    "\n",
    "    fig_val.add_trace(go.Violin(x0=name,\n",
    "                            y=error_data_test,\n",
    "                            legendgroup='Test',name='Test',\n",
    "                            side='positive',\n",
    "                            hovertext= list(np.concatenate(labels)),\n",
    "                            line_color='blue', box_visible=True)\n",
    "                 )\n",
    "    fig_val.add_trace(go.Violin(x0=name,\n",
    "                            y=error_data_train,\n",
    "                            legendgroup='Train',name='Train',\n",
    "                            side='negative',\n",
    "                            line_color='orange', box_visible=True)\n",
    "                 )\n",
    "    fig_val.update_traces(meanline_visible=True)\n",
    "    fig_val.update_layout(violingap=0, violinmode='overlay')\n",
    "    fig_val.update_traces(marker_opacity=0.75)\n",
    "    fig_val.update_layout(yaxis = dict(tickfont = dict(size=11)))\n",
    "    fig_val.update_layout(xaxis = dict(tickfont = dict(size=11)))\n",
    "    fig_val.update_yaxes(title_font=dict(size=22), color='black')\n",
    "    fig_val.update_xaxes(title_font=dict(size=22), color='black')\n",
    "    fig_val.update_layout(width=1000, height =1000)\n",
    "    fig_val.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "    fig_val.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True, showgrid=False)\n",
    "    fig_val.update_xaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "    fig_val.update_yaxes(ticks=\"inside\", tickwidth=1, tickcolor='black', ticklen=5)\n",
    "    fig_val.update_layout(yaxis = dict(tickfont = dict(size=18)))\n",
    "    fig_val.update_layout(yaxis_title=\"Validation Absolute error\")\n",
    "    fig_val.update_layout(yaxis = dict(tickfont = dict(size=18)))\n",
    "    fig_val.update_layout(xaxis = dict(tickfont = dict(size=18)))\n",
    "    fig_val.update_layout(template='simple_white')\n",
    "    fig_val.update_layout(yaxis_zeroline=False)\n",
    "    fig_val.write_html(\"{}/{}_validation.html\".format(modelname,modelname),include_mathjax = 'cdn')\n",
    "\n",
    "    \n",
    "    return fig_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_plot(model, X_train, iteration, modelname):\n",
    "    '''\n",
    "    Function to extract and store the shapley values plot to identify feature influence on model predictions \n",
    "    '''\n",
    "    \n",
    "    selected_feature_indices = np.argsort(-1*model.best_estimator_.steps[0][1].feature_importances_)[:\n",
    "                                model.best_estimator_.steps[0][1].n_features_to_select]\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    feature_names = [X_train.columns[i] for i in selected_feature_indices]\n",
    "    feature_score = [model.best_estimator_.steps[0][1].feature_importances_[i] \n",
    "              for i in selected_feature_indices]\n",
    "\n",
    "    #Extract shapley values from the best model\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model.best_estimator_.steps[1][1], X_train.filter(feature_names))\n",
    "    shap_values = explainer.shap_values(X_train.filter(feature_names), check_additivity=False)\n",
    "    \n",
    "    fig = shap.summary_plot(shap_values, features=X_train.filter(feature_names), feature_names=X_train.filter(feature_names).columns, \n",
    "                            show=False)\n",
    "    plt.savefig('{}/{}_{}.svg'.format(modelname,modelname,iteration))\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, param, X_train, y_train):\n",
    "    '''\n",
    "    Convenience function to setup inner cv pipeline for hyperparamter tuning \n",
    "    '''\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=18012019)\n",
    "    \n",
    "    #setup model training pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('selector', MultiSURFstar(n_jobs=-1)),\n",
    "        ('regressor', model)\n",
    "        ])\n",
    "    # define search\n",
    "    search = GridSearchCV(pipeline, param_grid=param, scoring='neg_mean_absolute_error', cv=cv_inner, refit=True, return_train_score=True, n_jobs=24)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    return result.best_estimator_, search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df(abs_errors, rmse_scores, r2_scores, mape_scores, model):\n",
    "    '''\n",
    "    Convenience function that evaluates cross validated model performance statistics and returns a pandas dataframe \n",
    "    '''\n",
    "    #define the dataframe\n",
    "    df = pd.DataFrame(index=[model])\n",
    "    \n",
    "    # map of metrics\n",
    "    metrics = {\n",
    "        'mae': abs_errors,\n",
    "        'max_error': abs_errors,\n",
    "        'rmse': rmse_scores,\n",
    "        'r2': r2_scores,\n",
    "        'mape': mape_scores\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def compute_stats(scores, metric_name):\n",
    "        '''\n",
    "        Wrapper function to evaluate statistics using numpy methods \n",
    "        '''\n",
    "        \n",
    "        if metric_name=='mae':\n",
    "            return {\n",
    "                'mean': np.mean(np.mean(scores, axis=1)),\n",
    "                'max': np.max(np.mean(scores, axis=1)),\n",
    "                'min': np.min(np.mean(scores, axis=1)),\n",
    "                'std': np.std(np.mean(scores, axis=1)),\n",
    "            }\n",
    "        \n",
    "        if metric_name=='max_error':\n",
    "            return {\n",
    "                'mean': np.mean(np.max(scores, axis=1)),\n",
    "                'max': np.max(scores),\n",
    "                'min': np.min(scores),\n",
    "                'std': np.std(np.max(scores, axis=1)),\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                'mean': np.mean(scores),\n",
    "                'max': np.max(scores),\n",
    "                'min': np.min(scores),\n",
    "                'std': np.std(scores)\n",
    "            }\n",
    "\n",
    "    #loop to calculate and populate the metrics in the dataframe\n",
    "    for metric, scores in metrics.items():\n",
    "        for subset in ['train', 'test']:\n",
    "            stats = compute_stats(scores=getattr(scores, subset),metric_name=metric)\n",
    "            for stat_name, value in stats.items():\n",
    "                df.loc[name, '{}_{}_{}'.format(metric,subset,stat_name)] = value    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735cb5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define convinience named tuples to store raw metric data of nested cv runs\n",
    "abs_errors = namedtuple(\"abs_errors\", \"train test\")\n",
    "rmse_scores = namedtuple(\"rmse_scores\", \"train test\")\n",
    "r2_scores = namedtuple(\"r2_scores\", \"train test\")\n",
    "mape_scores = namedtuple(\"mape_scores\", \"train test\")\n",
    "\n",
    "trained_models={}\n",
    "for name, (model, param) in zip(models_names, models_dicts.items()):\n",
    "    isExist = os.path.exists(name)\n",
    "    if not isExist:\n",
    "        os.mkdir(name)\n",
    "    print(name + ' model training')\n",
    "\n",
    "    abs_errors = namedtuple(\"abs_errors\", \"train test\")\n",
    "    rmse_scores = namedtuple(\"rmse_scores\", \"train test\")\n",
    "    r2_scores = namedtuple(\"r2_scores\", \"train test\")\n",
    "    mape_scores = namedtuple(\"mape_scores\", \"train test\")\n",
    "\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True, random_state=18012019)\n",
    "\n",
    "    #store outfold metrics\n",
    "    test_rmse = []\n",
    "    train_rmse =[]\n",
    "    test_r2 = []\n",
    "    train_r2 =[]\n",
    "    test_errors=[]\n",
    "    test_labels=[]\n",
    "    train_errors=[]\n",
    "    mape_train=[]\n",
    "    mape_test=[]\n",
    "    iteration=1\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix, :].values, X.iloc[test_ix, :].values\n",
    "        y_train, y_test = y.iloc[train_ix].values, y.iloc[test_ix].values\n",
    "        \n",
    "        #get the best model and gridsearch cv object\n",
    "        best_model, search = grid_search(model, param, X_train, y_train)\n",
    "        \n",
    "        # get train set predictions of best model\n",
    "        y_hat_train  = best_model.predict(X_train)\n",
    "        \n",
    "        # evaluate model on the hold out test dataset\n",
    "        y_hat_test = best_model.predict(X_test)\n",
    "        \n",
    "        # evaluate the model performace metrics on train and test sets\n",
    "        rmse_test = mean_squared_error(y_test, y_hat_test, squared=False)\n",
    "        rmse_train = mean_squared_error(y_train, y_hat_train, squared=False)\n",
    "\n",
    "        r2_test = r2_score(y_test, y_hat_test)\n",
    "        r2_train = r2_score(y_train, y_hat_train)\n",
    "\n",
    "        test_error = abs(y_test - y_hat_test)\n",
    "        train_error = abs(y_train - y_hat_train)\n",
    "\n",
    "        # store the result of each folds\n",
    "        test_rmse.append(rmse_test)\n",
    "        train_rmse.append(rmse_train)\n",
    "\n",
    "        test_r2.append(r2_test)\n",
    "        train_r2.append(r2_train)\n",
    "\n",
    "        test_errors.append(test_error)\n",
    "        test_labels.append(y.iloc[test_ix].index)\n",
    "        train_errors.append(train_error)\n",
    "\n",
    "        mape_train.append(mean_absolute_percentage_error(y_pred=y_hat_train, y_true=y_train))\n",
    "        mape_test.append(mean_absolute_percentage_error(y_pred=y_hat_test, y_true=y_test))\n",
    "\n",
    "        \n",
    "        # pickle the trained models  \n",
    "        filename = '{}/{}_bestmodel_{}.pkl'.format(name, name, iteration)\n",
    "        pickle.dump(best_model, open(filename, 'wb'))\n",
    "\n",
    "        #save shapley values plot for each fold\n",
    "        get_shap_plot(modelname=name,X_train=X.iloc[train_ix, :],model=search, iteration=iteration)\n",
    "\n",
    "        #save feature importance plot with scores from feature selection algorithm\n",
    "        get_feature_importance_plot(gridsearchcv_obj=search, modelname=name, features=X.iloc[train_ix, :], \n",
    "                                               iteration=iteration)\n",
    "\n",
    "        \n",
    "        print('>acc={}, est={}, cfg={}'.format(rmse_test, search.best_score_, search.best_params_))\n",
    "        print('Mean RMSE: {} (std : {})'.format(np.mean(test_rmse), np.std(test_rmse)))\n",
    "\n",
    "\n",
    "        iteration+=1\n",
    "\n",
    "    error_data_test = np.concatenate(test_errors)\n",
    "    error_data_train = np.concatenate(train_errors)\n",
    "\n",
    "    # store test and train absolute errors as violin+box plot\n",
    "    get_train_test_plot(errors_test=error_data_test, errors_train=error_data_train, \n",
    "                                  labels=test_labels,modelname=name)\n",
    "\n",
    "    # populate the defined named tuples with raw data of model performace\n",
    "    train_test_errors = abs_errors(train_errors, test_errors)\n",
    "    train_test_rmse = rmse_scores(train_rmse, test_rmse)\n",
    "    train_test_r2 = r2_scores(train_r2, test_r2)\n",
    "    train_test_mape = mape_scores(mape_train, mape_test)\n",
    "\n",
    "    # pass the named tuple to obtain summarized model performace metrics pandas dataframe\n",
    "    stats_df = get_metrics_df(abs_errors=train_test_errors, rmse_scores=train_test_rmse,\n",
    "                       r2_scores=train_test_r2, mape_scores=train_test_mape, model=name)\n",
    "\n",
    "    # save the summary stats as csv\n",
    "    stats_df.to_csv('summary_results.csv')\n",
    "    os.chdir('..')\n",
    "    \n",
    "    print(name + ' done')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
